{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes principales\n",
    "\n",
    "Supongamos que tenemos $n$ datos de una Normal $p$-dimensional $N_p(\\vec{\\mu},\\Sigma)$. Si desonocemos tanto $\\vec{\\mu}$ como $\\Sigma$, entonces debemos estimar $p+\\frac{p(p+1)}{2}$ parámetros diferentes (las $p$ entradas de $\\vec{\\mu}$ y las $\\frac{p(p+1)}{2}$ de $\\Sigma$). Si $p=5$, entonces son 20 parámetros. Y si $p=10$ ya tenemos 65 parámetros.\n",
    "\n",
    "Mientras más alto sea $p$, mucho mayor tiene que ser el número de observaciones para poder obtener estimaciones fiables.\n",
    "\n",
    "Hay varias técnicas de reducción de la dimensión que tratan de contestar la misma pregunta:\n",
    "¿Es posible describir con precisión los valores de las $p$ variables mediante un número menor de variables $r<p$?\n",
    "\n",
    "La respuesta es sí, y con ello **se habrá reducido la dimensión del problema a costa de una pequeña pérdida de información**.\n",
    "\n",
    "Vamos a ver dos técnicas:\n",
    "\n",
    "1) Análisis de Componentes Principales (PCA)\n",
    "\n",
    "2) Análisis Factorial (FA)\n",
    "\n",
    "Dadas $n$ observaciones de $p$ variables, se analiza si es posible representar adecuadamente esta información con un número menor de variables construidas como combinaciones lineales de las originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su utilidad es doble:\n",
    "\n",
    "1. Permite representar óptimamente en un espacio de dimensión pequeña, observaciones de un espacio general $p-$dimensional. En este sentido componentes principales es el primer paso para identificar posibles variables latentes no observadas, que están generando la variabilidad de los datos.\n",
    "\n",
    "2. Permite transformar las variables originales, en general correlacionadas, en nuevas variables incorrelacionadas, facilitando la interpretación de los datos.\n",
    "\n",
    "**El problema que se desea resolver es cómo encontrar un espacio de dimensión más reducida ($r<p$) que represente adecuadamente los datos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema puede abordarse desde tres perspectivas equivalentes:\n",
    "\n",
    "1) Enfoque descriptivo\n",
    "\n",
    "2) Enfoque estadístico\n",
    "\n",
    "3) Enfoque geométrico\n",
    "\n",
    "Nosotros nos enfocaremos en el enfoque (1) con el objetivo de entender el método de manera intuitiva pero **sin dejar de lado los enfoques (2) y (3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque descriptivo\n",
    "\n",
    "Se desea encontrar un subespacio de dimensión menor que $p$, tal que, al *proyectar los puntos sobre él, estos conserven su estructura con la menor distorsión posible*. \n",
    "\n",
    "Consideremos primero el caso de dos dimensiones ($p=2$) y un subespacio de dimensión uno: una recta.\n",
    "\n",
    "Se desea que las proyecciones de los puntos bidimensionales sobre esta recta mantengan, lo mejor posible, sus posiciones relativas.\n",
    "\n",
    "<img src=\"pca.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figura muestra los puntos en un diagrama de dispersión, y una recta. Esta recta, intuitivamente, proporciona un buen resumen de los datos, ya que las proyecciones de los puntos sobre ella indican aproximadamente la situación de los puntos en el plano. La representación es buena porque la recta pasa cerca de todos los puntos y estos se deforman poco al proyectarlos.\n",
    "\n",
    "Al proyectar cada punto sobre la recta se forma un triángulo rectángulo. La hipotenusa es la distancia del punto al origen $(x_i^Tx_i)^{1/2}$. Los catetos son la proyección del punto sobre la recta, $z_i$, y la distancia entre el punto y su proyección es $r_i$. Por el Teorema de Pitágoras tenemos $x_i^Tx_i=z_i^2+r_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumando sobre todos los puntos, tenemos $$\\sum_{i=1}^nx_i^Tx_i=\\sum_{i=1}^nz_i^2+\\sum_{i=1}^nr_i^2$$\n",
    "\n",
    "Minimizar la suma de las distancias a la recta de todos los puntos es equivalente a maximizar la suma al cuadrado de los valores de las proyecciones (pues la suma de las distancias de los puntos al origen es una constante).\n",
    "\n",
    "Ahora bien, maximizar la suma de sus cuadrados equivale a maximizar su varianza. Este resultado es intuitivo: la recta parece adecuada porque conserva lo más posible la variabilidad original de los puntos. Si consideramos una dirección de proyección perpendicular a la de la recta en esta figura, los puntos tendrían muy poca variabilidad y perderíamos la información sobre sus distancias en el espacio.\n",
    "\n",
    "En la figura anteriormente vista, **la recta no es la línea de regresión de ninguna de las variables con respecto a la otra**, que se obtienen minimizando las distancias verticales u horizontales, sino que al minimizar las distancias ortogonales o de proyección se encuentra entre ambas rectas de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De vuelta al caso general: cualquier $p\\ge 2$\n",
    "\n",
    "Este enfoque puede extenderse para obtener el mejor subespacio resumen de los datos de dimensión 2. Para ello calcularemos el plano que mejor aproxima a los puntos. Estadísticamente esto equivale a encontrar una segunda variable $z_2$, incorrelada con la anterior, y que tenga varianza máxima. \n",
    "\n",
    "En general, la componente  $r$, con $r<p$, tendrá varianza máxima entre todas las combinaciones lineales de las $p$ variables originales, con la condición de estar incorrelacionada con las $z_1,z_2,...,z_{r-1}$ obtenidas previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de las componentes\n",
    "\n",
    "El primer componente principal será la combinación lineal de las variables originales que tenga varianza máxima. Denotaremos esta primer componente principal por $Z_1$. Esta componente también es llamada *mejor recta de proyección de las variables originales*.\n",
    "\n",
    "Sea $X=(X_1,X_2,...,X_p)$ nuestra variable aleatoria multivariante **CON DATOS CENTRADOS** (es decir, restando su vector de medias).\n",
    "\n",
    "Así, $Z_1$ viene dada por $Z_1=a_{11}X_1+a_{12}X_2+...+a_{1p}X_p=\\langle X,\\vec{a_1}\\rangle$. Entonces $Var(Z_1)=\\langle \\vec{a_1},S\\vec{a_1}\\rangle$, donde $S$ es la matriz de covarianzas de las observaciones.\n",
    "\n",
    "Sin perder generalidad, supongamos que $\\|\\vec{a_1}\\|=1$ (es decir, $a_{11}^2+a_{12}^2+...+a_{1p}^2=1$. Se puede demostrar que la varianza de $Z_1$ es máxima cuando $\\vec{a_1}$ **es un vector propio de $S$, siendo $\\lambda$ su valor propio asociado**. Además, este $\\lambda$ es el valor propio de mayor valor de $S$ (para una demostración muy simple, ver [aquí](https://es.wikipedia.org/wiki/Norma_matricial); particularmente var la norma 2 de una matriz).\n",
    "\n",
    "En resumen, para calcular la primer componente principal:\n",
    "\n",
    "1) Calculamos la matriz de covarianzas de nuestros datos.\n",
    "\n",
    "2) Calculamos sus valores y vectories propios asociados.\n",
    "\n",
    "3) Elegimos el mayor valor propio. El vector asociado a ese valor, y de norma 1, nos da los coeficientes de la combinación lineal que definen a $Z_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento anterior puede generalizarse para demostrar que el espacio de dimensión $r$ que mejor representa a los puntos viene definido por los vectores propios asociados a los $r$ mayores autovalores de $S$. **Estas direcciones se denominan direcciones principales de los datos y a las nuevas variables definidas por esas direcciones se les llama componentes principales**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, la utilidad de PCA se puede ver de las siguientes formas:\n",
    "\n",
    "* Permite una representación óptima, en un espacio de dimensiones reducidas, de las observaciones originales.\n",
    "* Permite que las variables correlacionadas originales se transformen en nuevas variables no correlacionadas, facilitando la interpretación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades de los componentes principales\n",
    "\n",
    "1) Conservan la variabilidad inicial\n",
    "\n",
    "\n",
    "2) La proporción de variabilidad explicada por un componente es el cociente entre su varianza, el valor propio asociado al vector propio que lo define, y la suma de los valores propios de la matriz.\n",
    "\n",
    "\n",
    "## PCA normado\n",
    "\n",
    "Este PCA es idéntico al anterior, pero se aplica cuando las unidades de nuestras características son diferentes:\n",
    "\n",
    "1. Cuando las variables originales están en distintas unidades conviene aplicar el análisis de la matriz de correlaciones o análisis normado.\n",
    "\n",
    "2. Cuando las variables tienen las mismas unidades, ambas alternativas son posibles.\n",
    "\n",
    "3. Si las diferencias entre las varianzas de las variables son informativas y queremos tenerlas en cuenta en el análisis, no debemos estandarizar las variables.\n",
    "\n",
    "Por ejemplo, supongamos dos índices con la misma base, pero uno fluctúa mucho y el otro es casi constante. Este hecho es informativo, y para tenerlo en cuenta en el análisis, no se deben estandarizar las variables, de manera que el índice de mayor variabilidad tenga más peso.\n",
    "\n",
    "Por el contrario, si las diferencias de variabilidad no son relevantes podemos eliminarlas con el análisis normado. En caso de duda, conviene realizar ambos análisis, y seleccionar aquel que conduzca a conclusiones más informativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación\n",
    "\n",
    "En la práctica, los datos pueden ser representados por los “principal component scores”, que son los valores de las nuevas variables.\n",
    "\n",
    "Cuando existe una alta correlación positiva entre todas las variables, el primer componente principal tiene todas sus coordenadas del mismo signo y puede interpretarse como un promedio ponderado de todas las variables.\n",
    "\n",
    "Los resultados obtenidos en cada componente pueden interpretarse como medias ponderadas de dos grupos de variables con distinto signo y contraponen las variables de un signo a las del otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de los componentes\n",
    "\n",
    "Se han sugerido distintas reglas para seleccionar el número de componentes a mantener:\n",
    "\n",
    "\n",
    "1. Realizar un gráfico (scree-plot) de $\\lambda_i$ frente a $i$. Comenzar seleccionando componentes hasta que los restantes tengan aproximadamente el mismo valor de $\\lambda_i$. La idea es buscar un “codo” en el gráfico, es decir, un punto a partir del cual los valores propios son aproximadamente iguales. El criterio es quedarse con un número de componentes que excluya los asociados a valores pequeños y aproximadamente del mismo tamaño.\n",
    "\n",
    "\n",
    "2. Seleccionar componentes hasta cubrir una proporción determinada de varianza, como el 80\\% o el 90\\%. Esta regla es arbitraria y debe aplicarse con cierto cuidado. Por ejemplo, es posible que un único componente de “tamaño” recoja el 90\\% de la variabilidad y sin embargo pueden existir otros componentes que sean muy adecuados para explicar la “forma” las variables.\n",
    "\n",
    "\n",
    "3. Desechar aquellos componentes asociados a valores propios inferiores a una cota, que suele fijarse como la varianza media $\\frac{1}{p}\\sum\\lambda_i$. En particular, cuando se trabaja con la matriz de correlación $R$, el valor medio de los componentes es 1, y esta regla lleva a seleccionar los valores propios mayores que la unidad. De nuevo esta regla es arbitraria: una variable que sea independiente del resto puede tener un valor propio mayor que la unidad. Sin embargo, si está incorrelada con el resto puede ser una variable poco relevante para el análisis, y no aportar mucho a la comprensión del fenómeno global."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
