{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de factores\n",
    "\n",
    "El **análisis factorial** tiene por objetivo explicar un conjunto de variables observadas por un pequeño número de variables latentes, o no observadas, que llamaremos factores. \n",
    "\n",
    "Por ejemplo, supongamos que hemos tomado veinte medidas físicas del cuerpo de una persona: estatura, longitud del tronco y de las extremidades, anchura de hombros, peso, etc. Es intuitivo que todas estas medidas no son independientes entre sí, y que conocidas algunas de ellas podemos saber con poco error las restantes porque las dimensiones del cuerpo humano dependen de ciertos factores, y si estos fuesen conocidos podríamos prever con poco error los valores de las variables observadas.\n",
    "\n",
    "El análisis factorial está relacionado con los componentes principales. Sin embargo, una diferencia es que componentes principales es un herramienta descriptiva, mientras que el análisis factorial *presupone un modelo estadístico formal de generación de la muestra dada*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modelo factorial\n",
    "\n",
    "Sea $X=(X_1,X_2,...,X_p)$ nuestra variable multivariante. El modelo de análisis factorial establece que $X$ puede representarse de la siguiente manera:\n",
    " \n",
    "$$\\boldsymbol{X}=\\vec{\\boldsymbol{\\mu}}+\\Lambda \\boldsymbol{f}+\\boldsymbol{u}$$ \n",
    "\n",
    "donde:\n",
    "\n",
    "* $\\boldsymbol{f}$ es un vector aleatorio de dimensión $m$ ($m<p$) con vector de medias $\\vec{\\boldsymbol{0}}$ y matriz de covarianzas $I$ (es decir, sus componentes son uncorrelacionadas y tienen varianza 1). Este $\\boldsymbol{f}$ es llamado *vector de variables latentes* o *vector de factores no observados*. Es la información oculta que no podemos observar.\n",
    "\n",
    "\n",
    "* $\\Lambda$ es una matriz de $p$ filas y $m$ columnas, donde sus elementos son constantes que desconocemos. Contiene los coeficientes que describen cómo es que $\\boldsymbol{f}$ afecta al vector aleatorio original $\\boldsymbol{X}$. Se le conoce como *matriz de carga*.\n",
    "\n",
    "\n",
    "* $\\boldsymbol{u}$ es una variable aleatoria gaussiana multivariante con vector de medias $\\boldsymbol{0}$ y matriz de covarianzas $\\psi$, donde $\\psi$ es una matriz diagonal. Se trata de las perturbaciones no observadas (por ejemplo, el error que cometemos cuando usamos instrumentos no calibrados). Recoge el efecto de de todas las variables distintas de los factores que influyen sobre $\\boldsymbol{X}$. Supondremos también que $\\boldsymbol{u}$ y $\\boldsymbol{f}$ están incorrelacionados.\n",
    "\n",
    "\n",
    "* $\\vec{\\boldsymbol{\\mu}}$ es el vector de medias (poblacional) de $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamaremos $\\Sigma$ a la matriz de covarianzas de $\\boldsymbol{X}$.\n",
    "\n",
    "De esta manera, **estamos diciendo que cada dato de nuestra tabla es el resultado de la media de la característica a la que corresponde ese dato, junto con los factores escondidos que relacionan las características y perturbaciones que no podemos observar**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades\n",
    "\n",
    "Notemos que $X-\\vec{\\mu}=\\Lambda f+U$, de modo que $(X-\\vec{\\mu})f^T=\\Lambda ff^T+uf^T$. Tomando esperanza a ambos lados, se sigue que $E[(X-\\vec{\\mu})f^T]=\\Lambda$. Pero el lado izquierdo de esta última igualdad es $Cov(X-\\vec{mu},f)$. Por lo tanto $\\Lambda$ no es otra cosa mas que la matriz de covarianzas de $X-\\vec{\\mu}$ con $f$. En concreto: la entrada $\\Lambda_{ij}$ es la covarianza entre la componente $X_i$, de la multivariante $X$, y la componente $f_j$, del multivariante $f$.\n",
    "\n",
    "También se deduce la igualdad fundamental $$\\Sigma=\\Lambda\\Lambda^T+\\psi.$$\n",
    "\n",
    "Esta establece que la matriz de covarianzas de los datos observados admite una descomposición como suma de dos matrices.\n",
    "\n",
    "El primer sumando, la matriz $\\Lambda\\Lambda^T$, contiene la información sobre cómo los factores afectan la covarianza. Esta matriz contiene la parte común al conjunto de las variables y depende de las covarianzas entre las variables y los factores.\n",
    "\n",
    "El segundo sumando, la matriz $\\psi$ (recordemos que es una matriz diagonal), contiene la información sobre las perturbaciones en cada variable.\n",
    "\n",
    "Ahora bien... recordemos que $\\Sigma$ es la matriz de covarianzas de $X$. Los elementos de su diagonal corresponden a las varianzas de cada componente de $X$. Es decir, $\\sigma_i^2$.\n",
    "\n",
    "De $\\Sigma=\\Lambda\\Lambda^T+\\psi$ se sigue que $$\\sigma_i^2=\\sum_{j=1}^m\\Lambda_{ij}^2+\\psi^2_i$$\n",
    "\n",
    "La suma $\\sum_{j=1}^m\\Lambda_{ij}^2$ es llamada *i-ésima comunalidad* y se denota por $h_i^2$. El término $\\psi^2_i$ se conoce como *i-ésima unicidad*.\n",
    "\n",
    "Todo lo anterior se resume como **varianza observada = variabilidad común (comunalidad) + variabilidad específica (unicidad)**.\n",
    "\n",
    "Esto es análogo a la descomposición clásica de la variabilidad de los datos en una parte explicada y otra no explicada que se realiza en el análisis de la varianza.\n",
    "\n",
    "En el modelo factorial la parte explicada es debida a los factores y la no explicada es debido al ruido o componente aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicidad del modelo\n",
    "\n",
    "Observemos que tenemos un problema de indeterminación; a saber, si tenemos matrices de carga $\\Lambda$ y $\\Lambda^*$ y dos vectores de factores $f$ y $f^*$, ¿qué pareja $(\\Lambda,f)$ y $(\\Lambda,f^*)$ nos sirve?\n",
    "\n",
    "Aquí hay un matiz **muy importante**. En el modelo original, supusimos que los factores no están correlacionados entre sí. ¿Será que podemos encontrar una $f$ donde las correlaciones no sean 0 y siga explicando la misma información sobre $X$?\n",
    "\n",
    "La respuesta es sí; y de hecho, cualquier para cualquier conjunto de factores $f^*$ que explique a $X$ siempre existe un conjunto de factores $f$ con variables incorrelacionadas que también explica a $X$; y lo que es mejor: al revés también es cierto (es decir, si las variables de $f$ no tienen correlación, entonces siempre existe un conjunto de factores $f^*$ con variables correlacionadas que explica exactamente la misma información sobre $X$).\n",
    "\n",
    "En resumen, la restricción sobre la incorrelacionalidad de los factores no es en realidad tan fuerte. Supongamos pues, como antes, que la matriz de covarianza de $f$ es la identidad.\n",
    "\n",
    "Ahora bien, es sencillo demostrar que si $H$ es una matriz ortogonal, entonces tomando $\\Lambda^*=\\Lambda H$ y $f^*=H^Tf$, el modelo de factores es exactamente el mismo. Esto significa el modelo factorial está indeterminado ante rotaciones. O sea que no podemos distinguir modelos \"cuando hay matrices ortogonales de por medio\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varimax\n",
    "\n",
    "La interpretación de los factores se facilita si los que afectan a algunas variables no lo hacen a otras y al revés. Este objetivo conduce al criterio de maximizar la varianza de los coeficientes que definen los efectos de cada factor sobre las variables observadas.\n",
    "\n",
    "Bajo este criterio, se selecciona la llamada *matriz de varimax* $H^*$, que es una matriz ortogonal, la cual tiene la propiedad de que si $\\Lambda$ es la matriz de carga, entonces $\\Lambda^*=\\Lambda H^*$ es una matriz de carga que hace que haya factores con correlaciones altas con un número pequeño de variables y correlaciones nulas en el resto, quedando así redistribuida la varianza de los factores..\n",
    "\n",
    "En R, se utiliza la función ``varimax`` para calcularla.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
