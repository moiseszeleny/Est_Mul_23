{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a algoritmos de clasificación\n",
    "\n",
    "Clasificar datos es asignar a las observaciones un grupo determinado asumiendo que los datos son heterogéneos y se pueden dividir en grupos. El problema es el siguiente:\n",
    "\n",
    "* Tenemos un conjunto de elementos que provienen de dos o más poblaciones.\n",
    "\n",
    "* Observamos una variable $p$-dimensional $X=(X_1,X_2,...,X_p)$ en estos elementos.\n",
    "\n",
    "* Queremos clasificar un nuevo elemento, con valores conocidos para esta variable pero desconocemos de cuál población proviene.\n",
    "\n",
    "## Definición del problema\n",
    "\n",
    "Tenemos una variable aleatoria $p$-dimensional $X=(X_1,X_2,...,X_p)$ definida sobre elementos muestrales que pertenecen a $G$ poblaciones, $P_g$, para $g=1,2,...,G$. La variable respuesta $Y$ contiene las clasificaciones de los elementos muestrales, es decir, toma valores en $g=1,2,...,G$.\n",
    "\n",
    "Tenemos una matriz de datos $\\boldsymbol{X}$ de tamaño $n\\times p$ con observaciones $\\boldsymbol{x_i}$ para $i=1,2,...,n$.\n",
    "\n",
    "Desconocemos las probabilidades $\\pi_g$ para $g=1,2,...,G$, de que un elemento seleccionado al azar provenga de la población $g$. Se cumple que $\\pi_1+\\pi_2+...+\\pi_G=1$. Queremos clasificar un nuevo elemento con valores conocidos $\\boldsymbol{x_0}=(x_{01},x_{02},...,x_{0p})$ dentro de alguna población $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchas técnicas de clasificación para abordar este problema. Vamos a estudiar las tres técnicas más conocidas:\n",
    "\n",
    "1) $K$ vecinos más cercanos (kNN: k-Nearest Neighbors)\n",
    "\n",
    "2) Clasificador bayesiano\n",
    "\n",
    "3) Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN: $K$-Nearest Neighbors o $K$ vecinos más cercanos\n",
    "\n",
    "Es un método de clasificación no paramétrico; es decir, **no requiere asumir ninguna distribución para la variable aleatoria $X=(X_1,X_2,...,X_p)$**. Este método no requiere estimar las probabilidades desconocidas $\\pi_g$ de que un elemento seleccionado al azar provenga de la población $g$.\n",
    "\n",
    "La idea es buscar, para la nueva observación que queremos clasificar, sus $K$ vecinos más cercanos, es decir, las $K$ observaciones más cercanas respecto a una medida de distancia.\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "1) Definimos una medida de distancia adecuada para las observaciones.\n",
    "\n",
    "2) Calculamos la distancia entre la nueva observación $\\boldsymbol{x_0}$ que queremos clasificar, y las observaciones que tenemos en nuestra matriz de datos.\n",
    "\n",
    "3) Seleccionamos las $K$ observaciones más cercanas a $\\boldsymbol{x_0}$, y miramos a qué grupo pertenecen.\n",
    "\n",
    "4) Clasificamos $\\boldsymbol{x_0}$ en la población a la que pertenece una mayor proporción de sus $K$ vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación bayesiana\n",
    "\n",
    "La idea es: clasificar o asignar la nueva observación a la población con mayor probabilidad de haberla generado. En otras palabras, clasificamos a $\\boldsymbol{x_0}$ en la población $g$-ésima $P_g$ si $P(y_0=g|x=\\boldsymbol{x_0})$ es la probabilidad máxima dentro de todas las posibles considerando el total de $G$ poblaciones.\n",
    "\n",
    "Supongamos que cada uno de los $G$ grupos son gaussianos. Sea $f_g(\\cdot;\\theta_g)$ su respectiva densidad (es decir, $f_g$ es una gaussiana $p$ dimensional).\n",
    "\n",
    "Entonces $$P(y_0=g|x=\\boldsymbol{x_0})=\\frac{\\pi_gf_g(\\boldsymbol{x_0})}{\\sum_{i=1}^G\\pi_if_i(\\boldsymbol{x_0})}$$\n",
    "\n",
    "1) Si suponemos que $\\mu_1,\\mu_2,...,\\mu_G$ no son todos iguales, pero tienen la misma matriz de covarianza, se dice que se hace un **análisis discriminante lineal.**\n",
    "\n",
    "2) Por el contrario, si suponemos que $\\mu_1,\\mu_2,...,\\mu_G$ no son todos igualesy cada población tiene su propia matriz de covarianza, se dice que se hace un **análisis discriminante cuadrático.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística\n",
    "\n",
    "Uno de los problemas de la clasificación bayesiana es que se asume que la variable aleatoria tiene una distribución específica en la población $P_g$. Esto restringe el tipo de variables que pueden usarse en las reglas discriminantes lineal y cuadrática. Sin embargo, un marco probabilístico aporta un soporte muy fuerte a las decisiones; la pregunta es si se podrían calcular las probabilidades $P(y_0=g|x=\\boldsymbol{x_0})$ sin tener conocimiento a priori de las densidades.\n",
    "\n",
    "Consideremos el problema de discriminación o clasificación entre dos poblaciones. Y supongamos que además de las variables explicativas tenemos una variable respuesta que toma valor cero para una de esas poblaciones y valor uno para la otra. En caso de que sean clases no numéricas siempre se pueden transformar a estos valores.\n",
    "\n",
    "El método más usual para llevar a cabo regresión logística es el **multilogit** (cuando hay mas de dos poblaciones).\n",
    "\n",
    "Aquí, $$P(y_0=g|x=\\boldsymbol{x_0})=\\frac{e^{\\beta_{0g}+\\beta_{1g}\\boldsymbol{x_0}}}{1+\\sum_{i=1}^{G-1} e^{\\beta_{0i}+\\beta_{1i}\\boldsymbol{x_0}}}$$ para $g=1,2,...,G-1$ y $$P(y_0=G|x=\\boldsymbol{x_0})=\\frac{1}{1+\\sum_{i=1}^{G-1} e^{\\beta_{0i}+\\beta_{1i}\\boldsymbol{x_0}}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Así, el modelo logístico puede aplicarse cuando las variables explicativas no son Normales, incluyendo variables discretas y variables categóricas que pueden incluirse en el modelo vía variables dummy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
